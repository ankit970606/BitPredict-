# -*- coding: utf-8 -*-
"""BitPredict ðŸ’°ðŸ“ˆ

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EIKFL9W2INKw5eqv68lFR08YZ4vJCKhW
"""



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# STEP 1: LOAD DATA AND ADD TECHNICAL INDICATORS
# ============================================================================

file_path = '/content/Bitcoin_5_13_2010-7_12_2010_historical_data_coinmarketcap.csv'
df = pd.read_csv(file_path, sep=';')

# Clean data
df = df.apply(lambda x: x.str.replace('"', '') if x.dtype == "object" else x)
numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'marketCap', 'circulatingSupply']
for col in numeric_columns:
    df[col] = pd.to_numeric(df[col])

df['timestamp'] = pd.to_datetime(df['timestamp'])
df = df.sort_values('timestamp').reset_index(drop=True)

print("="*80)
print("STEP 1: DATA LOADING & FEATURE ENGINEERING")
print("="*80)
print(f"Dataset: {len(df)} records from {df['timestamp'].min()} to {df['timestamp'].max()}")

# ============================================================================
# STEP 2: ADD TECHNICAL INDICATORS
# ============================================================================

def add_technical_indicators(df):
    """Calculate technical indicators from price data"""

    # Simple Moving Averages
    df['SMA_7'] = df['close'].rolling(window=7).mean()
    df['SMA_14'] = df['close'].rolling(window=14).mean()
    df['SMA_30'] = df['close'].rolling(window=30).mean()

    # Exponential Moving Average
    df['EMA_12'] = df['close'].ewm(span=12, adjust=False).mean()
    df['EMA_26'] = df['close'].ewm(span=26, adjust=False).mean()

    # MACD (Moving Average Convergence Divergence)
    df['MACD'] = df['EMA_12'] - df['EMA_26']
    df['MACD_signal'] = df['MACD'].ewm(span=9, adjust=False).mean()

    # RSI (Relative Strength Index)
    delta = df['close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))

    # Bollinger Bands
    df['BB_middle'] = df['close'].rolling(window=20).mean()
    bb_std = df['close'].rolling(window=20).std()
    df['BB_upper'] = df['BB_middle'] + (bb_std * 2)
    df['BB_lower'] = df['BB_middle'] - (bb_std * 2)
    df['BB_width'] = (df['BB_upper'] - df['BB_lower']) / df['BB_middle']

    # Price momentum
    df['momentum'] = df['close'].pct_change(periods=10)

    # Volatility
    df['volatility'] = df['close'].rolling(window=10).std()

    # Volume indicators
    df['volume_MA'] = df['volume'].rolling(window=7).mean()
    df['volume_ratio'] = df['volume'] / df['volume_MA']

    # Price changes
    df['daily_return'] = df['close'].pct_change()
    df['price_range'] = (df['high'] - df['low']) / df['close']

    return df

df = add_technical_indicators(df)

print("\nTechnical Indicators Added:")
print("- Moving Averages: SMA (7, 14, 30), EMA (12, 26)")
print("- MACD & Signal Line")
print("- RSI (14-day)")
print("- Bollinger Bands")
print("- Momentum & Volatility")
print("- Volume Indicators")

# ============================================================================
# STEP 3: ADD SENTIMENT DATA (SIMULATED FOR HISTORICAL DATA)
# ============================================================================

def add_sentiment_features(df):
    """
    Add sentiment features. In production, this would come from:
    - Twitter API (tweepy) for social sentiment
    - News APIs (NewsAPI, CryptoCompare) for news sentiment
    - Fear & Greed Index API

    For historical data, we simulate sentiment based on price action
    """

    # Simulate market sentiment based on price momentum
    # In reality, you'd use NLP on actual tweets/news

    # Price-based sentiment (simplified)
    returns = df['close'].pct_change(7)
    df['price_sentiment'] = np.where(returns > 0.05, 1,  # Bullish
                                     np.where(returns < -0.05, -1,  # Bearish
                                             0))  # Neutral

    # Volatility-based fear index (high volatility = high fear)
    volatility_normalized = (df['volatility'] - df['volatility'].min()) / (df['volatility'].max() - df['volatility'].min())
    df['fear_index'] = volatility_normalized * 100

    # Volume surge indicator (social media activity proxy)
    df['volume_surge'] = (df['volume'] > df['volume_MA'] * 1.5).astype(int)

    # Trend strength (how strong is the current trend)
    df['trend_strength'] = abs(df['close'] - df['SMA_30']) / df['SMA_30']

    return df

df = add_sentiment_features(df)

print("\nSentiment Features Added (Simulated):")
print("- Price Sentiment (-1: Bearish, 0: Neutral, 1: Bullish)")
print("- Fear Index (0-100)")
print("- Volume Surge Indicator")
print("- Trend Strength")
print("\nâš ï¸  NOTE: For real predictions, integrate actual APIs:")
print("   - Twitter API for tweet sentiment")
print("   - NewsAPI for news sentiment")
print("   - Alternative.me for Fear & Greed Index")

# ============================================================================
# STEP 4: PREPARE FEATURES FOR LSTM
# ============================================================================

# Drop NaN values from technical indicators
df = df.dropna().reset_index(drop=True)

print(f"\nData after adding features: {len(df)} records")

# Select comprehensive feature set
feature_columns = [
    # Price features
    'open', 'high', 'low', 'close', 'volume',

    # Technical indicators
    'SMA_7', 'SMA_14', 'SMA_30', 'EMA_12', 'EMA_26',
    'MACD', 'MACD_signal', 'RSI', 'BB_width',
    'momentum', 'volatility', 'volume_ratio',
    'daily_return', 'price_range',

    # Sentiment features
    'price_sentiment', 'fear_index', 'volume_surge', 'trend_strength'
]

data = df[feature_columns].values

print(f"\n{'='*80}")
print("FEATURE SELECTION")
print(f"{'='*80}")
print(f"Total features: {len(feature_columns)}")
print(f"Features: {feature_columns}")

# ============================================================================
# STEP 5: NORMALIZE DATA
# ============================================================================

scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

close_scaler = MinMaxScaler(feature_range=(0, 1))
close_scaler.fit(df[['close']].values)

print("\nâœ“ All features normalized to [0, 1]")

# ============================================================================
# STEP 6: CREATE SEQUENCES
# ============================================================================

def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(seq_length, len(data)):
        X.append(data[i-seq_length:i])
        y.append(data[i, 3])  # Index 3 = 'close' price
    return np.array(X), np.array(y)

SEQUENCE_LENGTH = 60
X, y = create_sequences(scaled_data, SEQUENCE_LENGTH)

print(f"\n{'='*80}")
print("SEQUENCE CREATION")
print(f"{'='*80}")
print(f"Lookback period: {SEQUENCE_LENGTH} days")
print(f"X shape: {X.shape} (samples, timesteps, features)")
print(f"y shape: {y.shape}")

# ============================================================================
# STEP 7: TRAIN/TEST SPLIT
# ============================================================================

train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

print(f"\nTraining samples: {len(X_train)}")
print(f"Testing samples: {len(X_test)}")

# ============================================================================
# STEP 8: BUILD ENHANCED LSTM MODEL
# ============================================================================

print(f"\n{'='*80}")
print("BUILDING ENHANCED LSTM MODEL")
print(f"{'='*80}")

model = Sequential([
    # Bidirectional LSTM for better pattern recognition
    Bidirectional(LSTM(units=100, return_sequences=True),
                  input_shape=(X_train.shape[1], X_train.shape[2])),
    Dropout(0.3),

    # Second LSTM layer
    LSTM(units=100, return_sequences=True),
    Dropout(0.3),

    # Third LSTM layer
    LSTM(units=50, return_sequences=False),
    Dropout(0.2),

    # Dense layers
    Dense(units=25, activation='relu'),
    Dropout(0.2),

    # Output layer
    Dense(units=1)
])

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='huber',  # Huber loss is more robust to outliers
    metrics=['mae', 'mse']
)

print("\nModel Architecture:")
model.summary()

# ============================================================================
# STEP 9: TRAIN MODEL
# ============================================================================

print(f"\n{'='*80}")
print("TRAINING MODEL WITH ENHANCED FEATURES")
print(f"{'='*80}")

early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=0.00001)

history = model.fit(
    X_train, y_train,
    epochs=150,
    batch_size=32,
    validation_split=0.1,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

# ============================================================================
# STEP 10: PREDICTIONS & EVALUATION
# ============================================================================

print(f"\n{'='*80}")
print("MAKING PREDICTIONS")
print(f"{'='*80}")

predictions = model.predict(X_test)
predictions_actual = close_scaler.inverse_transform(predictions)
y_test_actual = close_scaler.inverse_transform(y_test.reshape(-1, 1))

# Metrics
mse = mean_squared_error(y_test_actual, predictions_actual)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test_actual, predictions_actual)
mape = np.mean(np.abs((y_test_actual - predictions_actual) / y_test_actual)) * 100

print(f"\nMODEL PERFORMANCE:")
print(f"  RMSE: ${rmse:.2f}")
print(f"  MAE:  ${mae:.2f}")
print(f"  MAPE: {mape:.2f}%")

# ============================================================================
# STEP 11: VISUALIZATIONS
# ============================================================================

print(f"\n{'='*80}")
print("GENERATING VISUALIZATIONS")
print(f"{'='*80}")

# Plot 1: Training History
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2)
axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)
axes[0, 0].set_title('Model Loss', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

axes[0, 1].plot(history.history['mae'], label='Train MAE', linewidth=2)
axes[0, 1].plot(history.history['val_mae'], label='Val MAE', linewidth=2)
axes[0, 1].set_title('Mean Absolute Error', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('MAE')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# Predictions vs Actual
axes[1, 0].plot(y_test_actual, label='Actual', color='blue', linewidth=2)
axes[1, 0].plot(predictions_actual, label='Predicted', color='red', linewidth=2, alpha=0.7)
axes[1, 0].set_title('Price Predictions vs Actual', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('Time Steps')
axes[1, 0].set_ylabel('Price ($)')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# Error distribution
error = predictions_actual.flatten() - y_test_actual.flatten()
axes[1, 1].hist(error, bins=50, color='purple', alpha=0.7, edgecolor='black')
axes[1, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)
axes[1, 1].set_title('Prediction Error Distribution', fontsize=14, fontweight='bold')
axes[1, 1].set_xlabel('Error ($)')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Plot 2: Feature Importance Visualization
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

# Technical indicators
axes[0, 0].plot(df['timestamp'][-200:], df['close'][-200:], label='Close', linewidth=2)
axes[0, 0].plot(df['timestamp'][-200:], df['SMA_7'][-200:], label='SMA 7', alpha=0.7)
axes[0, 0].plot(df['timestamp'][-200:], df['SMA_30'][-200:], label='SMA 30', alpha=0.7)
axes[0, 0].set_title('Price with Moving Averages', fontsize=14, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# RSI
axes[0, 1].plot(df['timestamp'][-200:], df['RSI'][-200:], color='orange', linewidth=2)
axes[0, 1].axhline(y=70, color='red', linestyle='--', label='Overbought')
axes[0, 1].axhline(y=30, color='green', linestyle='--', label='Oversold')
axes[0, 1].set_title('RSI Indicator', fontsize=14, fontweight='bold')
axes[0, 1].set_ylim(0, 100)
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# Sentiment
axes[1, 0].plot(df['timestamp'][-200:], df['fear_index'][-200:], color='red', linewidth=2)
axes[1, 0].set_title('Fear Index (Simulated)', fontsize=14, fontweight='bold')
axes[1, 0].set_ylabel('Fear Level')
axes[1, 0].grid(True, alpha=0.3)

# Volume with surge indicators
axes[1, 1].bar(df['timestamp'][-200:], df['volume'][-200:], alpha=0.5, label='Volume')
surge_dates = df['timestamp'][-200:][df['volume_surge'][-200:] == 1]
surge_volumes = df['volume'][-200:][df['volume_surge'][-200:] == 1]
axes[1, 1].scatter(surge_dates, surge_volumes, color='red', s=50, label='Volume Surge', zorder=5)
axes[1, 1].set_title('Volume with Surge Detection', fontsize=14, fontweight='bold')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ============================================================================
# STEP 12: PREDICT NEXT DAY
# ============================================================================

print(f"\n{'='*80}")
print("TOMORROW'S PRICE PREDICTION")
print(f"{'='*80}")

last_sequence = scaled_data[-SEQUENCE_LENGTH:].reshape(1, SEQUENCE_LENGTH, X_train.shape[2])
next_day_prediction = model.predict(last_sequence, verbose=0)
next_day_price = close_scaler.inverse_transform(next_day_prediction)

current_price = df['close'].iloc[-1]
predicted_price = next_day_price[0][0]
change = predicted_price - current_price
change_pct = (change / current_price) * 100

print(f"\nCurrent Price (Today):     ${current_price:.2f}")
print(f"Predicted Price (Tomorrow): ${predicted_price:.2f}")
print(f"Expected Change:            ${change:+.2f} ({change_pct:+.2f}%)")
print(f"\nPrediction: {'ðŸ“ˆ BULLISH' if change > 0 else 'ðŸ“‰ BEARISH'}")

# Display current market conditions
print(f"\n{'='*80}")
print("CURRENT MARKET CONDITIONS")
print(f"{'='*80}")
print(f"RSI:              {df['RSI'].iloc[-1]:.2f} {'(Overbought)' if df['RSI'].iloc[-1] > 70 else '(Oversold)' if df['RSI'].iloc[-1] < 30 else '(Neutral)'}")
print(f"MACD:             {df['MACD'].iloc[-1]:.4f}")
print(f"Volatility:       {df['volatility'].iloc[-1]:.2f}")
print(f"Fear Index:       {df['fear_index'].iloc[-1]:.2f}/100")
print(f"Trend Strength:   {df['trend_strength'].iloc[-1]:.4f}")

# Save model
model.save('bitcoin_enhanced_lstm_model.h5')
print(f"\nâœ“ Model saved as 'bitcoin_enhanced_lstm_model.h5'")

print(f"\n{'='*80}")
print("ðŸŽ¯ ANALYSIS COMPLETE!")
print(f"{'='*80}")
print("\nðŸ“Š Model trained with:")
print(f"   â€¢ {len(feature_columns)} features (price + technical + sentiment)")
print(f"   â€¢ Bidirectional LSTM architecture")
print(f"   â€¢ {len(X_train)} training samples")
print(f"   â€¢ RMSE: ${rmse:.2f}, MAPE: {mape:.2f}%")
print("\nâš ï¸  IMPORTANT: For REAL predictions with actual sentiment:")
print("   1. Sign up for Twitter API (sentiment analysis)")
print("   2. Get NewsAPI key (news sentiment)")
print("   3. Use Alternative.me API (Fear & Greed Index)")
print("   4. Integrate real-time data feeds")
print(f"{'='*80}\n")

